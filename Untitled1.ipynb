{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdoYXL_Pe1as",
        "outputId": "d6fb96d8-3f76-400e-f1d2-b1c4ed6179dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "%pip install --upgrade nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/extracted_entitiesab.csv\")\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to preprocess text (tokenization, lemmatization, punctuation removal)\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text\n",
        "    word_tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "\n",
        "    # Lemmatize and remove punctuation\n",
        "    cleaned_text = [lemmatizer.lemmatize(word) for word in word_tokens if word not in string.punctuation]\n",
        "\n",
        "    return ' '.join(cleaned_text)\n",
        "\n",
        "# Apply preprocessing to 'Description' column\n",
        "df['Cleaned_Description'] = df['Description'].apply(lambda x: preprocess_text(x) if pd.notna(x) else '')\n",
        "\n",
        "# Preview the cleaned text\n",
        "print(df[['Description', 'Cleaned_Description']].head())\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load pre-trained spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract named entities\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Apply NER to the cleaned descriptions\n",
        "df['Entities'] = df['Cleaned_Description'].apply(extract_entities)\n",
        "\n",
        "# Preview the data with extracted entities\n",
        "print(df[['Description', 'Cleaned_Description', 'Entities']].head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMHzQq_bTkeB",
        "outputId": "84060b7d-ed79-4200-fc8f-5fe060e84fc1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Entity Label  \\\n",
            "0  INTERNATIONAL APPLICATION PUBLISHED UNDER THE ...   ORG   \n",
            "1                                                PCT   ORG   \n",
            "2  World Intellectual Property Organization Inter...   ORG   \n",
            "3                   International Publication Number   ORG   \n",
            "4                  International Publication Date WO   ORG   \n",
            "\n",
            "                                         Description  \n",
            "0  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...  \n",
            "1  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...  \n",
            "2  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...  \n",
            "3  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...  \n",
            "4  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         Description  \\\n",
            "0  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "1  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "2  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "3  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "4  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "\n",
            "                                 Cleaned_Description  \n",
            "0  12 international application published under t...  \n",
            "1  12 international application published under t...  \n",
            "2  12 international application published under t...  \n",
            "3  12 international application published under t...  \n",
            "4  12 international application published under t...  \n",
            "                                         Description  \\\n",
            "0  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "1  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "2  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "3  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "4  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "\n",
            "                                 Cleaned_Description  \\\n",
            "0  12 international application published under t...   \n",
            "1  12 international application published under t...   \n",
            "2  12 international application published under t...   \n",
            "3  12 international application published under t...   \n",
            "4  12 international application published under t...   \n",
            "\n",
            "                                            Entities  \n",
            "0  [(12, CARDINAL), (19, CARDINAL), (10, CARDINAL...  \n",
            "1  [(12, CARDINAL), (19, CARDINAL), (10, CARDINAL...  \n",
            "2  [(12, CARDINAL), (19, CARDINAL), (10, CARDINAL...  \n",
            "3  [(12, CARDINAL), (19, CARDINAL), (10, CARDINAL...  \n",
            "4  [(12, CARDINAL), (19, CARDINAL), (10, CARDINAL...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Function to get sentiment polarity (positive, negative, neutral)\n",
        "def get_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    return analysis.sentiment.polarity  # Sentiment score (-1 to 1)\n",
        "\n",
        "# Apply sentiment analysis\n",
        "df['Sentiment'] = df['Cleaned_Description'].apply(get_sentiment)\n",
        "\n",
        "# Preview sentiment scores\n",
        "print(df[['Description', 'Sentiment']].head())\n",
        "\n",
        "df.to_csv(\"processed_entities.csv\", index=False)\n",
        "\n",
        "print(\"Processed data saved to 'processed_entities.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJeD_mH3UOeU",
        "outputId": "7d11c99b-e409-4efb-b637-72d4129d616e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         Description  Sentiment\n",
            "0  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   0.037500\n",
            "1  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   0.075705\n",
            "2  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   0.037500\n",
            "3  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   0.037500\n",
            "4  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   0.037500\n",
            "Processed data saved to 'processed_entities.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "\n",
        "def summarize_text(text, num_sentences=2):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "\n",
        "    word_frequencies = {}\n",
        "    for word in doc:\n",
        "        if word.text.lower() not in STOP_WORDS and word.text.lower() not in punctuation:\n",
        "            word_frequencies[word.text.lower()] = word_frequencies.get(word.text.lower(), 0) + 1\n",
        "\n",
        "    max_frequency = max(word_frequencies.values())\n",
        "    for word in word_frequencies:\n",
        "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
        "\n",
        "    sentence_scores = {}\n",
        "    for sent in doc.sents:\n",
        "        for word in sent:\n",
        "            if word.text.lower() in word_frequencies:\n",
        "                sentence_scores[sent] = sentence_scores.get(sent, 0) + word_frequencies[word.text.lower()]\n",
        "\n",
        "    summary_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
        "    summary = \" \".join([str(sent) for sent in summary_sentences])\n",
        "    return summary"
      ],
      "metadata": {
        "id": "4ByfHSSZ5ur2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaR6KENMHTOp",
        "outputId": "1c202b9d-71d2-405e-c8ac-3c5b9508b7bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/processed_entities.csv')\n",
        "print(data.head())\n",
        "\n",
        "def LemTokens(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n",
        "\n",
        "data['Description'] = data['Description'].fillna('').astype(str)\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "preprocessed_sentences = [LemNormalize(sentence) for sentence in data['Description']]\n",
        "preprocessed_sentences_str = [' '.join(sentence) for sentence in preprocessed_sentences]\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def process_user_query(user_query):\n",
        "    query_processed = LemNormalize(user_query)\n",
        "    query_str = ' '.join(query_processed)\n",
        "    query_embedding = model.encode([query_str])\n",
        "    if not isinstance(query_embedding, np.ndarray):\n",
        "        query_embedding = np.array(query_embedding)\n",
        "    return query_embedding\n",
        "\n",
        "user_query = input(\"Enter your query (or type 'exit' to quit): \")\n",
        "\n",
        "query_embedding = process_user_query(user_query).astype(np.float32)\n",
        "sentence_embeddings = model.encode(preprocessed_sentences_str).astype(np.float32)\n",
        "\n",
        "\n",
        "print(\"Sentence embeddings:\", sentence_embeddings)\n",
        "print(\"Query Embedding:\", query_embedding)\n"
      ],
      "metadata": {
        "id": "TLq1GAXPHbHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef17107-8a91-490d-ef0d-3e40a0a85efe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Entity Label  \\\n",
            "0  INTERNATIONAL APPLICATION PUBLISHED UNDER THE ...   ORG   \n",
            "1                                                PCT   ORG   \n",
            "2  World Intellectual Property Organization Inter...   ORG   \n",
            "3                   International Publication Number   ORG   \n",
            "4                  International Publication Date WO   ORG   \n",
            "\n",
            "                                         Description  \\\n",
            "0  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "1  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "2  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "3  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "4  (12) INTERNATIONAL APPLICATION PUBLISHED UNDER...   \n",
            "\n",
            "                                 Cleaned_Description  \\\n",
            "0  12 international application published under t...   \n",
            "1  12 international application published under t...   \n",
            "2  12 international application published under t...   \n",
            "3  12 international application published under t...   \n",
            "4  12 international application published under t...   \n",
            "\n",
            "                                            Entities  Sentiment  \n",
            "0  [('12', 'CARDINAL'), ('19', 'CARDINAL'), ('10'...   0.037500  \n",
            "1  [('12', 'CARDINAL'), ('19', 'CARDINAL'), ('10'...   0.075705  \n",
            "2  [('12', 'CARDINAL'), ('19', 'CARDINAL'), ('10'...   0.037500  \n",
            "3  [('12', 'CARDINAL'), ('19', 'CARDINAL'), ('10'...   0.037500  \n",
            "4  [('12', 'CARDINAL'), ('19', 'CARDINAL'), ('10'...   0.037500  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query (or type 'exit' to quit): uav\n",
            "Sentence embeddings: [[-0.06737203  0.04754766  0.00790684 ... -0.07381594 -0.06231912\n",
            "   0.00258845]\n",
            " [-0.09162535  0.11541863 -0.02089767 ...  0.00305743 -0.0275792\n",
            "   0.04578074]\n",
            " [-0.06737202  0.04754765  0.00790682 ... -0.07381594 -0.06231911\n",
            "   0.00258848]\n",
            " ...\n",
            " [-0.06515896  0.06558628 -0.01483229 ...  0.09415536 -0.08263117\n",
            "  -0.07280099]\n",
            " [-0.06515896  0.06558628 -0.01483229 ...  0.09415536 -0.08263117\n",
            "  -0.07280099]\n",
            " [-0.04509854  0.0688794   0.04400564 ... -0.06060551 -0.0402807\n",
            "   0.01775094]]\n",
            "Query Embedding: [[ 2.84795910e-02  2.18813345e-02 -4.45201285e-02 -4.89867991e-03\n",
            "  -4.24737968e-02  8.17566738e-03  4.04779091e-02 -6.02429360e-02\n",
            "  -3.79130952e-02  1.25582978e-01  6.87636733e-02  1.79092139e-02\n",
            "   3.67567055e-02  3.73019017e-02 -1.30240032e-02  1.08865352e-04\n",
            "  -1.22875040e-02 -9.63344425e-02 -2.84022167e-02  2.54526343e-02\n",
            "  -5.06005697e-02  1.16227604e-01 -2.56736372e-02  1.65580902e-02\n",
            "  -4.55573760e-02  3.79199646e-02 -1.24750584e-02  5.94625399e-02\n",
            "  -3.02562546e-02 -1.15604043e-01  6.88512204e-03  3.20609249e-02\n",
            "   9.03728232e-02 -1.07664326e-02 -1.15932785e-02 -1.62150227e-02\n",
            "   3.94026414e-02 -9.10954550e-03 -5.67989759e-02  1.49935232e-02\n",
            "  -1.18645653e-02 -4.23604026e-02  8.51404145e-02  3.32288630e-02\n",
            "  -2.78573390e-02  2.71157492e-02  5.74045535e-03  2.46766880e-02\n",
            "   1.51451200e-01  5.94653375e-02 -2.12692264e-02  1.83521006e-02\n",
            "  -4.06706147e-02 -3.84296128e-03 -4.47184592e-03 -9.39577520e-02\n",
            "  -1.43538313e-02  2.74409167e-02  9.80849285e-03 -7.16538951e-02\n",
            "   6.31957315e-04  1.22698165e-01 -1.11518139e-02 -1.36633823e-03\n",
            "  -2.42796224e-02 -5.43470122e-02 -3.53633314e-02 -6.66655526e-02\n",
            "   2.22780295e-02 -1.00192957e-01 -6.37994185e-02  3.37360092e-02\n",
            "  -2.10374035e-02  7.50290975e-02 -5.58478683e-02  1.60766374e-02\n",
            "   1.15811743e-01  2.34171115e-02  4.81231362e-02 -3.13241370e-02\n",
            "   3.97921018e-02 -2.80451626e-02 -1.36962414e-01 -7.72584509e-03\n",
            "   3.19838785e-02 -2.39281692e-02  2.43131584e-03  8.40217993e-02\n",
            "   3.06626298e-02  4.13799807e-02  2.93904264e-02 -4.26579490e-02\n",
            "   9.95178968e-02 -1.19135482e-02  1.01357875e-02 -2.87864003e-02\n",
            "  -4.22381572e-02 -1.11114383e-01 -2.08809711e-02  9.24942940e-02\n",
            "   2.97572985e-02 -1.12227604e-01 -5.20200580e-02  4.88033742e-02\n",
            "  -1.57006904e-02  8.43653455e-03  4.32954542e-02 -4.11140686e-03\n",
            "   2.86795013e-02 -4.42614220e-02 -7.89614674e-03  5.91986142e-02\n",
            "  -6.89208508e-02  5.59692970e-03 -4.71050553e-02  6.03205897e-02\n",
            "  -1.00517713e-01 -1.15937628e-02 -2.99498457e-02  5.13927825e-03\n",
            "  -5.97629398e-02 -6.29974231e-02  4.48469557e-02 -3.84535082e-02\n",
            "   3.23086814e-03 -8.06256160e-02  4.69184183e-02 -3.10372863e-33\n",
            "  -9.66344178e-02 -5.59972040e-02 -4.02825102e-02  1.94814671e-02\n",
            "   1.01736961e-02 -7.11237639e-03  5.01187965e-02 -2.39763055e-02\n",
            "  -1.98297128e-02 -2.40968354e-02 -7.17362240e-02  1.94966253e-02\n",
            "   1.25690494e-02  4.24138233e-02  1.50576398e-01 -4.02701348e-02\n",
            "   9.91310272e-03 -6.76446408e-02 -2.71620918e-02 -1.66029502e-02\n",
            "  -5.93331689e-03 -2.14761496e-02 -3.66320387e-02  2.01239083e-02\n",
            "   8.93670768e-02 -1.91444140e-02 -5.37736006e-02 -3.26712206e-02\n",
            "   5.00922501e-02  5.51087782e-02 -3.25236954e-02  9.25663412e-02\n",
            "  -7.17194080e-02 -3.00586578e-02 -6.50276104e-03  8.53083469e-03\n",
            "  -5.75302690e-02 -4.38102745e-02 -9.31363255e-02  2.88319308e-02\n",
            "  -2.10215319e-02  2.71402253e-03 -3.09776738e-02  3.24407145e-02\n",
            "   2.10207161e-02 -7.19410367e-03  2.78714187e-02  1.84810441e-03\n",
            "  -1.03706852e-01  4.16228771e-02 -2.97984499e-02  6.49214238e-02\n",
            "  -5.35257831e-02 -6.64422065e-02 -6.42384663e-02 -2.74024270e-02\n",
            "   5.06453551e-02  1.43393204e-02 -4.22557294e-02 -1.77293792e-02\n",
            "   5.81228212e-02  4.09965254e-02 -1.21640787e-02 -2.64689792e-02\n",
            "  -4.30241711e-02 -1.04355179e-01  1.05845898e-01 -5.16591221e-03\n",
            "   9.59403440e-02  2.79636402e-02 -2.32861321e-02  4.94249985e-02\n",
            "   6.83221966e-02 -7.36128390e-02 -1.78286042e-02  1.63089409e-02\n",
            "  -4.30023447e-02 -4.70043952e-03  4.31076484e-03 -2.29880530e-02\n",
            "  -5.71095571e-02  3.16982642e-02 -7.96940457e-03  6.17232658e-02\n",
            "   5.82042113e-02  5.65253347e-02 -4.24311720e-02 -4.04484607e-02\n",
            "   1.94336474e-02 -2.37949099e-02 -7.91713595e-02  7.87068252e-03\n",
            "  -1.14390929e-03 -7.28945201e-03  1.36481859e-02  1.98242402e-33\n",
            "   1.65437814e-02 -1.15024000e-02  9.56193823e-03  4.58277166e-02\n",
            "  -2.05964930e-02 -2.46475227e-02  7.83921853e-02  5.72194532e-02\n",
            "  -9.13579762e-02  1.50718549e-02 -7.25913346e-02 -1.20955585e-02\n",
            "   4.77237515e-02 -2.15050392e-03  8.61245170e-02 -8.88387486e-03\n",
            "   3.81990410e-02 -3.71731981e-03  6.28167465e-02  2.20585335e-02\n",
            "  -1.95163749e-02 -5.07094711e-03  5.25971018e-02 -9.77583081e-02\n",
            "   5.05529009e-02 -2.95111984e-02  3.49893235e-02  6.01914525e-02\n",
            "  -1.25459544e-02  7.41366157e-03  2.98739634e-02 -6.63061738e-02\n",
            "   9.48829297e-03  5.63324466e-02 -1.39638560e-03  3.33185717e-02\n",
            "   1.05274782e-01  1.91373378e-02 -5.90558648e-02 -3.09769958e-02\n",
            "   8.67378861e-02  1.12789478e-02 -3.10161412e-02  2.38438975e-02\n",
            "  -4.29905653e-02  2.96687111e-02  4.30933572e-02  8.93962979e-02\n",
            "   4.04427247e-03 -5.81776677e-03  1.68669168e-02 -3.52280810e-02\n",
            "  -1.88187172e-03  4.29553017e-02  1.71024594e-02 -2.00518104e-03\n",
            "   7.81832170e-03  3.09802908e-02  5.28984927e-02  3.07443514e-02\n",
            "   4.20038924e-02  1.49469301e-02 -6.46497235e-02 -2.82718358e-03\n",
            "  -5.91674782e-02  6.30485415e-02  6.34090155e-02  4.48639318e-02\n",
            "  -5.40241040e-02  3.49862278e-02 -6.39958587e-03 -3.90570089e-02\n",
            "   1.51817156e-02 -3.70485373e-02 -6.63336962e-02 -5.36827520e-02\n",
            "  -4.30215336e-02 -2.13494655e-02  1.82302967e-02 -1.02317585e-02\n",
            "   2.65417732e-02 -8.49092603e-02 -4.24358957e-02  7.15278611e-02\n",
            "  -2.01292634e-02  8.12599286e-02  2.87078861e-02 -3.84645313e-02\n",
            "  -4.38426062e-02  7.59573933e-03 -7.25392252e-02 -4.15665517e-03\n",
            "   6.05686419e-02 -8.73505417e-03 -7.84487799e-02 -1.17665300e-08\n",
            "  -5.21910153e-02  8.66335928e-02 -3.74156907e-02  6.20022137e-03\n",
            "   3.86940055e-02  7.20596388e-02 -4.61471081e-02 -2.67950017e-02\n",
            "   5.56690954e-02  6.85937353e-04  3.20524089e-02 -1.07529350e-01\n",
            "  -1.97196547e-02  4.66499627e-02  2.03162935e-02  4.75572571e-02\n",
            "  -7.84300268e-03  2.43967008e-02  5.93624823e-03 -6.09126464e-02\n",
            "  -2.39168350e-02 -1.86518617e-02 -6.17224090e-02  1.19273722e-01\n",
            "  -7.50896260e-02  8.80087540e-02 -6.40956387e-02 -1.27936369e-02\n",
            "   6.82209581e-02  3.53414118e-02 -1.80700663e-02  7.81114250e-02\n",
            "  -3.80180404e-02 -6.27728626e-02 -1.42104011e-02 -1.61014292e-02\n",
            "   2.62989961e-02 -2.65247617e-02  2.32865233e-02 -2.47444604e-02\n",
            "   8.00096095e-02  2.34576571e-03  3.00934929e-02 -1.61545705e-02\n",
            "   5.67186810e-02  3.12850848e-02  3.15622054e-02 -1.30723640e-01\n",
            "  -7.32168183e-02 -5.37823066e-02  7.47490972e-02  1.22070601e-02\n",
            "  -1.96652487e-02  1.59319758e-01  4.02823091e-02  5.39890565e-02\n",
            "   7.21215159e-02 -1.32420510e-01  1.18745898e-03  8.82919729e-02\n",
            "   6.18293285e-02  3.79516147e-02 -1.75930932e-02  9.83779319e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_scores = cosine_similarity(query_embedding, sentence_embeddings)\n",
        "\n",
        "top_n = 10\n",
        "top_n_indices = np.argsort(similarity_scores[0])[::-1][:top_n]\n",
        "\n",
        "print(f\"Top {top_n} most relevant sentences for your query:\")\n",
        "for idx in top_n_indices:\n",
        "    print(f\"Sentence: {preprocessed_sentences_str[idx]}\")\n",
        "    print(f\"Similarity score: {similarity_scores[0][idx]}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "1X3F7mddK1Cr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4928816-617c-410b-def8-9eb5e622ef3c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most relevant sentences for your query:\n",
            "Sentence: the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in realtime by control command re ceived via the wireless command link 115\n",
            "Similarity score: 0.5411809682846069\n",
            "--------------------------------------------------------------------------------\n",
            "Sentence: the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in realtime by control command re ceived via the wireless command link 115\n",
            "Similarity score: 0.5411809682846069\n",
            "--------------------------------------------------------------------------------\n",
            "Sentence: the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in realtime by control command re ceived via the wireless command link 115\n",
            "Similarity score: 0.5411809682846069\n",
            "--------------------------------------------------------------------------------\n",
            "Sentence: the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in realtime by control command re ceived via the wireless command link 115\n",
            "Similarity score: 0.5411809682846069\n",
            "--------------------------------------------------------------------------------\n",
            "Sentence: the uav 100 may be controlled in an autonomous mode wherein it fly according to a primary route r1 r1 ’ defined by a first set of predefined waypoints wp1wp8 ip\n",
            "Similarity score: 0.5321874618530273\n",
            "--------------------------------------------------------------------------------\n",
            "Sentence: the uav 100 may be controlled in an autonomous mode wherein it fly according to a primary route r1 r1 ’ defined by a first set of predefined waypoints wp1wp8 ip\n",
            "Similarity score: 0.5321874618530273\n",
            "--------------------------------------------------------------------------------\n",
            "Sentence: the invention relates to remote control of an un manned aerial vehicle uav 100 from a control station 110 by mean of a wireless command link 115 the uav 100 may be controlled in an autonomous mode wherein it fly according to a primary route r1 r1 ’ defined by a first set of predefined waypoints wp1wp8 ip the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in realtime by control command re ceived via the wireless command link 115 flight control parameter are monitored in both mode and in case a major alarm condition occurs the uav 100 is controlled to follow an emergency route r2 ’ defined by a second set of predefined waypoints hp1hp7 tp1tp9 ip then the emergency route r2 ’ involves flying the uav 100 to an air space above a termination waypoint tp9 on the ground at which it is estimated that the vehicle ’ s 100 flight may be ended without injuring any personnel or causing uncontrolled material damage\n",
            "Similarity score: 0.5311759114265442\n",
            "--------------------------------------------------------------------------------\n",
            "Sentence: the invention relates to remote control of an un manned aerial vehicle uav 100 from a control station 110 by mean of a wireless command link 115 the uav 100 may be controlled in an autonomous mode wherein it fly according to a primary route r1 r1 ’ defined by a first set of predefined waypoints wp1wp8 ip the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in realtime by control command re ceived via the wireless command link 115 flight control parameter are monitored in both mode and in case a major alarm condition occurs the uav 100 is controlled to follow an emergency route r2 ’ defined by a second set of predefined waypoints hp1hp7 tp1tp9 ip then the emergency route r2 ’ involves flying the uav 100 to an air space above a termination waypoint tp9 on the ground at which it is estimated that the vehicle ’ s 100 flight may be ended without injuring any personnel or causing uncontrolled material damage\n",
            "Similarity score: 0.5311759114265442\n",
            "--------------------------------------------------------------------------------\n",
            "Sentence: this publication describes a method for remotely controlling an aerial vehicle\n",
            "Similarity score: 0.5254250764846802\n",
            "--------------------------------------------------------------------------------\n",
            "Sentence: the uav 100 may be controlled in an autonomous mode wherein it fly according to a primary route r1 r1 ’ defined by a first set of predefined waypoints wp1wp8 ip the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in realtime by control command re ceived via the wireless command link 115\n",
            "Similarity score: 0.5195391178131104\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MajTZLKKZ49W",
        "outputId": "7dde4a01-af9a-4ffa-9048-a99d5d9c8e37"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity_embeddings = model.encode(data['Entity'].tolist())\n",
        "entity_embeddings = np.array(entity_embeddings).astype('float32')\n",
        "\n",
        "import faiss\n",
        "index = faiss.IndexFlatL2(entity_embeddings.shape[1])\n",
        "index.add(entity_embeddings)\n",
        "\n",
        "\n",
        "def search_with_faiss(query_embedding, index, k=5):\n",
        "    D, I = index.search(np.array(query_embedding).astype('float32'), k)\n",
        "    return I, D\n",
        "\n",
        "top_indices, distances = search_with_faiss(query_embedding, index)\n",
        "\n",
        "for idx in top_indices[0]:\n",
        "  print(f\"Entity: {data.iloc[idx]['Entity']}, Label: {data.iloc[idx]['Label']}, Description: {data.iloc[idx]['Cleaned_Description']}\")"
      ],
      "metadata": {
        "id": "bvmd2iMyaCrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c16a5b-59b6-43d8-901f-d53a54034049"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: UAV, Label: ORG, Description: the invention relates to remote control of an un- manned aerial vehicle uav 100 from a control station 110 by mean of a wireless command link 115 the uav 100 may be controlled in an autonomous mode wherein it fly according to a primary route r1 r1 ’ defined by a first set of predefined waypoints wp1-wp8 ip the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in real-time by control command re- ceived via the wireless command link 115 flight control parameter are monitored in both mode and in case a major alarm condition occurs the uav 100 is controlled to follow an emergency route r2 ’ defined by a second set of predefined waypoints hp1-hp7 tp1-tp9 ip then the emergency route r2 ’ involves flying the uav 100 to an air space above a termination waypoint tp9 on the ground at which it is estimated that the vehicle ’ s 100 flight may be ended without injuring any personnel or causing uncontrolled material damage\n",
            "Entity: UAV, Label: ORG, Description: the invention relates to remote control of an un- manned aerial vehicle uav 100 from a control station 110 by mean of a wireless command link 115 the uav 100 may be controlled in an autonomous mode wherein it fly according to a primary route r1 r1 ’ defined by a first set of predefined waypoints wp1-wp8 ip the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in real-time by control command re- ceived via the wireless command link 115 flight control parameter are monitored in both mode and in case a major alarm condition occurs the uav 100 is controlled to follow an emergency route r2 ’ defined by a second set of predefined waypoints hp1-hp7 tp1-tp9 ip then the emergency route r2 ’ involves flying the uav 100 to an air space above a termination waypoint tp9 on the ground at which it is estimated that the vehicle ’ s 100 flight may be ended without injuring any personnel or causing uncontrolled material damage\n",
            "Entity: The UAV, Label: ORG, Description: the uav 100 may be controlled in an autonomous mode wherein it fly according to a primary route r1 r1 ’ defined by a first set of predefined waypoints wp1-wp8 ip the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in real-time by control command re- ceived via the wireless command link 115\n",
            "Entity: The UAV, Label: ORG, Description: the uav 100 may be controlled in an autonomous mode wherein it fly according to a primary route r1 r1 ’ defined by a first set of predefined waypoints wp1-wp8 ip the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in real-time by control command re- ceived via the wireless command link 115\n",
            "Entity: the UAV, Label: ORG, Description: flight control parameter are monitored in both mode and in case a major alarm condition occurs the uav 100 is controlled to follow an emergency route r2 ’ defined by a second set of predefined waypoints hp1-hp7 tp1-tp9 ip then the emergency route r2 ’ involves flying the uav 100 to an air space above a termination waypoint tp9 on the ground at which it is estimated that the vehicle ’ s 100 flight may be ended without injuring any personnel or causing uncontrolled material damage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "# Load the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def summarizy_text(text, num_sentences=2):\n",
        "    # Summarize text using the model\n",
        "    summary = summarizer(text, max_length=250, min_length=75, do_sample=False, length_penalty=2.0)\n",
        "\n",
        "    summary_text = summary[0]['summary_text']\n",
        "\n",
        "    # Check if the summary ends abruptly and add a conclusion if needed\n",
        "    if not summary_text.endswith(('.', '!', '?')):\n",
        "        summary_text += '.'\n",
        "\n",
        "    return summary_text\n",
        "\n",
        "distances, indices = index.search(query_embedding, k=1)\n",
        "\n",
        "if len(indices[0]) > 0 and indices[0][0] != -1:\n",
        "    idx = indices[0][0]\n",
        "    relevant_text = data.iloc[idx]['Entity']\n",
        "    label = data.iloc[idx]['Label']\n",
        "    Desc = data.iloc[idx]['Cleaned_Description']\n",
        "\n",
        "\n",
        "    summary = summarizy_text(Desc, num_sentences=2)\n",
        "\n",
        "    print(\"\\n--- Relevant Document ---\")\n",
        "    print(f\"Entity: {relevant_text}\")\n",
        "    print(f\"Label: {label}\")\n",
        "    print(f\"Description: {Desc}\")\n",
        "    print(\"\\n--- Summary ---\")\n",
        "    print(summary)\n",
        "else:\n",
        "    print(\"No relevant documents found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR_27kuhjaOT",
        "outputId": "4cc99ec7-3e76-458f-e821-c4ed35f91d6b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 250, but your input_length is only 223. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Relevant Document ---\n",
            "Entity: UAV\n",
            "Label: ORG\n",
            "Description: the invention relates to remote control of an un- manned aerial vehicle uav 100 from a control station 110 by mean of a wireless command link 115 the uav 100 may be controlled in an autonomous mode wherein it fly according to a primary route r1 r1 ’ defined by a first set of predefined waypoints wp1-wp8 ip the uav 100 may also be controlled in a manual mode wherein it fly according to an alternative primary route r1 ’ defined in real-time by control command re- ceived via the wireless command link 115 flight control parameter are monitored in both mode and in case a major alarm condition occurs the uav 100 is controlled to follow an emergency route r2 ’ defined by a second set of predefined waypoints hp1-hp7 tp1-tp9 ip then the emergency route r2 ’ involves flying the uav 100 to an air space above a termination waypoint tp9 on the ground at which it is estimated that the vehicle ’ s 100 flight may be ended without injuring any personnel or causing uncontrolled material damage\n",
            "\n",
            "--- Summary ---\n",
            "The invention relates to remote control of an un- manned aerial vehicle uav 100 from a control station 110 by mean of a wireless command link 115. The Uav 100 may be controlled in an autonomous mode wherein it fly according to a primary route r1 r1 ’ defined by a first set of predefined waypoints wp1-WP8 ip. The uav100 may also be controlled to follow an emergency route r2 r2 which involves flying the vehicle to an air space above a termination waypoint tp9.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "pgMA-u7N7wrS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response (user_response, data):\n",
        "  robo1_response = ''\n",
        "  descriptions = data['Cleaned_Description'].tolist()\n",
        "  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english' )\n",
        "  tfidf = TfidfVec.fit_transform(descriptions)\n",
        "  user_query_normalized = LemNormalize(user_response)\n",
        "  user_query_str = ' '.join(user_query_normalized)\n",
        "  user_query_tfidf = TfidfVec.transform([user_query_str])\n",
        "  vals = cosine_similarity(user_query_tfidf, tfidf)\n",
        "  idx = vals.argsort ()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat. sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if (req_tfidf == 0):\n",
        "    robo1_response = robo1_response + \"I am sorry. Unable to understand you! Please be more clear.\"\n",
        "  else:\n",
        "    robo1_response += data.iloc[idx]['Entity'] + \": \" + data.iloc[idx]['Cleaned_Description']\n",
        "    return robo1_response"
      ],
      "metadata": {
        "id": "us9TmrZG8OzI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install schedule\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpqdDI-v8gNU",
        "outputId": "4e95fa5e-02c9-4430-9da9-d433e8ce0e9a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting schedule\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(query_embedding))\n",
        "print(type(sentence_embeddings))\n",
        "print(query_embedding.shape)\n",
        "print(sentence_embeddings.shape)\n",
        "query_embedding = query_embedding.reshape(1, -1)  # Ensure it's a 2D array\n",
        "sentence_embeddings = np.array(sentence_embeddings)  # Ensure it's a 2D numpy array\n",
        "print(type(query_embedding))\n",
        "print(type(sentence_embeddings))\n",
        "print(query_embedding.shape)\n",
        "print(sentence_embeddings.shape)\n",
        "\n",
        "# Ensure all embeddings are numeric and 2D\n",
        "if not np.issubdtype(query_embedding.dtype, np.floating):\n",
        "    raise ValueError(\"query_embedding contains non-numeric values\")\n",
        "if not np.issubdtype(sentence_embeddings.dtype, np.floating):\n",
        "    raise ValueError(\"sentence_embeddings contains non-numeric values\")\n",
        "\n",
        "if query_embedding.ndim != 2 or sentence_embeddings.ndim != 2:\n",
        "        raise ValueError(\"Both query_embedding and sentence_embeddings must be 2D arrays\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCxznJsurURb",
        "outputId": "4d558fe4-e384-49ab-b916-54bc37d5281e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "(1, 384)\n",
            "(2368, 384)\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "(1, 384)\n",
            "(2368, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import queue\n",
        "import threading\n",
        "import schedule\n",
        "import time\n",
        "\n",
        "conversation_context = {\"last_query\": \"\", \"last_response\": \"\"}\n",
        "unanswered_queries = []\n",
        "\n",
        "def process_user_query(user_query):\n",
        "    query_processed = LemNormalize(user_query)\n",
        "    query_str = ' '.join(query_processed)\n",
        "    query_embedding = model.encode([query_str])\n",
        "    if not isinstance(query_embedding, np.ndarray):\n",
        "        query_embedding = np.array(query_embedding)\n",
        "    return query_embedding\n",
        "\n",
        "def log_unanswered_query(query):\n",
        "    with open(\"unanswered_questions.json\", \"a\") as f:\n",
        "        f.write(json.dumps({\"query\": query}) + \"\\n\")\n",
        "\n",
        "def user_feedback(response):\n",
        "    print(f\"Bot: {response}\")\n",
        "    feedback = input(\"Was this response helpful? (yes/no): \").strip().lower()\n",
        "    return feedback\n",
        "\n",
        "def log_feedback(query, response, feedback):\n",
        "    feedback_data = {\n",
        "        \"query\": query,\n",
        "        \"response\": response,\n",
        "        \"feedback\": feedback\n",
        "    }\n",
        "    with open(\"feedback.json\", \"a\") as f:\n",
        "        f.write(json.dumps(feedback_data) + \"\\n\")\n",
        "\n",
        "greet_inputs = ('hello', 'hey', 'hi', 'whassup', 'how are you?','Namasate','Good Morning')\n",
        "def greet(user_response):\n",
        "    for word in user_response.split():\n",
        "      if word.lower() in greet_inputs:\n",
        "        return \"Hello! How can I assist you today?\"\n",
        "    return None\n",
        "\n",
        "def find_relevant_document(query_embedding, sentence_embeddings, data):\n",
        "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
        "\n",
        "    similarity = cosine_similarity(query_embedding, sentence_embeddings)\n",
        "\n",
        "    best_idx = np.argmax(similarity)\n",
        "    best_score = similarity[0][best_idx]\n",
        "    if best_score > 0.5:\n",
        "        best_match = data.iloc[best_idx]['Cleaned_Description']\n",
        "        return best_match, best_score\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "\n",
        "def summarizy_text(response, num_sentences=2):\n",
        "    # Summarize text using the model\n",
        "    summary = summarizer(response, max_length=250, min_length=75, do_sample=False, length_penalty=2.0)\n",
        "\n",
        "    summary_text = summary[0]['summary_text']\n",
        "\n",
        "    # Check if the summary ends abruptly and add a conclusion if needed\n",
        "    if not summary_text.endswith(('.', '!', '?')):\n",
        "        summary_text += '.'\n",
        "\n",
        "    return summary_text\n",
        "\n",
        "\n",
        "def chatbot(user_input_queue):\n",
        "    print('Hello! I am BeeB0T. Start typing your text after greeting me. For ending the convo type bye!')\n",
        "\n",
        "    flag = True\n",
        "    while flag:\n",
        "        user_response = user_input_queue.get()\n",
        "        user_response = user_response.lower()\n",
        "\n",
        "        if user_response in ['thank you', 'thanks']:\n",
        "            print('Bot: You are Welcome...')\n",
        "            flag = False\n",
        "            break\n",
        "\n",
        "        if user_response.startswith((\"what about\", \"tell me more\", \"can you explain\")):\n",
        "            if conversation_context[\"last_response\"]:\n",
        "                print(\"--- Follow-Up Response ---\")\n",
        "                print(\"Let me expand on that...\")\n",
        "                print(conversation_context[\"last_response\"])\n",
        "            else:\n",
        "                print(\"I'm not sure what you're referring to. Can you ask a specific question?\")\n",
        "            continue\n",
        "\n",
        "        greeting_response = greet(user_response)\n",
        "        if greeting_response:\n",
        "            print('Bot:', greeting_response)\n",
        "        else:\n",
        "            query_embedding = process_user_query(user_response)\n",
        "            response, doc_idx = find_relevant_document(query_embedding, sentence_embeddings, data)\n",
        "\n",
        "            if response:\n",
        "\n",
        "                feedback = user_feedback(response)\n",
        "                log_feedback(user_response, response, feedback)\n",
        "\n",
        "                conversation_context[\"last_query\"] = user_response\n",
        "                conversation_context[\"last_response\"] = response\n",
        "            else:\n",
        "                print(\"I'm sorry, I couldn't find relevant information.\")\n",
        "                print(\"Can you clarify or provide more details?\")\n",
        "\n",
        "                unanswered_queries.append(user_response)\n",
        "                log_unanswered_query(user_response)\n",
        "                print(\"Your query has been logged for further analysis. Thank you for helping me improve!\")\n",
        "\n",
        "\n",
        "def review_unanswered_queries(filename='unanswered_questions.json'):\n",
        "    with open(filename, 'r') as file:\n",
        "        unanswered_queries = [json.loads(line) for line in file.readlines()]\n",
        "    if unanswered_queries:\n",
        "        for query in unanswered_queries:\n",
        "            print(f\"Reviewing query: {query['query']}\")\n",
        "            response, doc_idx = find_relevant_document(query['query'], None, None)\n",
        "\n",
        "            if doc_idx is None:\n",
        "                print(\"Still no relevant document found for this query.\")\n",
        "            else:\n",
        "                summary = summarizy_text(response)\n",
        "                print(f\"Answer found: {response}\")\n",
        "                print(f\"Summary: {summary}\")\n",
        "                query['status'] = 'answered'\n",
        "\n",
        "        with open(filename, 'w') as file:\n",
        "            for query in unanswered_queries:\n",
        "                file.write(json.dumps(query) + \"\\n\")\n",
        "    else:\n",
        "        print(\"No unanswered queries!\")\n",
        "\n",
        "\n",
        "def run_scheduled_tasks():\n",
        "    while True:\n",
        "        schedule.run_pending()\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "def main():\n",
        "    user_input_queue = queue.Queue()\n",
        "\n",
        "    chatbot_thread = threading.Thread(target=chatbot, args=(user_input_queue,))\n",
        "    chatbot_thread.start()\n",
        "\n",
        "    scheduled_thread = threading.Thread(target=run_scheduled_tasks)\n",
        "    scheduled_thread.start()\n",
        "\n",
        "    schedule.every().day.at(\"11:00\").do(review_unanswered_queries)\n",
        "\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Message BeeBOT: \")\n",
        "        user_input_queue.put(user_input)\n",
        "        if user_input.lower() in ['bye', 'exit']:\n",
        "          print('Bot: Goodbye!')\n",
        "          break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n",
        "import os\n",
        "def retrain_model(new_queries):\n",
        "    # Load existing patent texts\n",
        "    patent_texts = load_existing_patent_texts()\n",
        "\n",
        "    # Extend with new queries\n",
        "    patent_texts.extend(new_queries)\n",
        "\n",
        "    # Preprocess the patent texts (normalization, tokenization, etc.)\n",
        "    preprocessed_patents = preprocess_text(patent_texts)\n",
        "\n",
        "    # Initialize the SentenceTransformer model (can be replaced with any other model if needed)\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Encode patent texts into embeddings\n",
        "    patent_embeddings = model.encode(preprocessed_patents)\n",
        "\n",
        "    # Save the retrained embeddings to a file for future use\n",
        "    np.save(\"retrained_patent_embeddings.npy\", patent_embeddings)\n",
        "\n",
        "    # Optionally, save the updated patent texts for reference\n",
        "    np.save(\"retrained_patent_texts.npy\", patent_texts)\n",
        "\n",
        "    print(\"Model retrained successfully and embeddings saved!\")\n",
        "\n",
        "def load_existing_patent_texts(file_path='/content/processed_entities.csv'):\n",
        "    try:\n",
        "        # Read the CSV file and convert the 'text' column to a list\n",
        "        patent_texts = pd.read_csv(file_path)['Cleaned_Description'].tolist()\n",
        "        return patent_texts\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading patent texts: {e}\")\n",
        "        return []\n",
        "\n",
        "def LemNormalize(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n",
        "\n",
        "def preprocess_text(text_data):\n",
        "    return [' '.join(LemNormalize(text)) for text in text_data]\n",
        "\n",
        "def load_unanswered_queries(file_path=\"unanswered_questions.json\"):\n",
        "    try:\n",
        "          with open(file_path, 'r') as file:\n",
        "              unanswered_queries = json.load(file)\n",
        "          return unanswered_queries\n",
        "    except Exception as e:\n",
        "          print(f\"Error loading unanswered queries: {e}\")\n",
        "          return []\n",
        "\n",
        "# If there are unanswered queries, retrain the model with them\n",
        "unanswered_queries = load_unanswered_queries()\n",
        "if unanswered_queries:\n",
        "    retrain_model(unanswered_queries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMaJ1mQf807f",
        "outputId": "e9a35827-a831-4b5e-ccf4-662fcc9fc260"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! I am BeeB0T. Start typing your text after greeting me. For ending the convo type bye!\n",
            "Message BeeBOT: hi\n",
            "Bot: Hello! How can I assist you today?\n",
            "Message BeeBOT: uav copter\n",
            "Bot: these publication describe a drone control system\n",
            "Was this response helpful? (yes/no): yes\n",
            "Message BeeBOT: exit\n"
          ]
        }
      ]
    }
  ]
}